{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MS COCO Dataset Download and traininng.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXVjZrNIsps9Nt3IaZ5sCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shafaqulhamim/MS-coco-training/blob/main/MS_COCO_Dataset_Download_and_traininng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU-okL35ElFo",
        "outputId": "64b00c69-465e-4fae-d19f-2239d66146c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opencv-python-headless==4.5.4.60\n",
            "  Downloading opencv_python_headless-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.5.4.60) (1.21.6)\n",
            "Installing collected packages: opencv-python-headless\n",
            "Successfully installed opencv-python-headless-4.5.4.60\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python-headless==4.5.4.60"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CxPoM7ME3wP",
        "outputId": "2fa1c1ad-e6bb-48ca-e537-d48b2206b624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fiftyone\n",
            "  Downloading fiftyone-0.16.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 15.8 MB/s \n",
            "\u001b[?25hCollecting eventlet\n",
            "  Downloading eventlet-0.33.1-py2.py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 50.3 MB/s \n",
            "\u001b[?25hCollecting fiftyone-brain<0.9,>=0.8\n",
            "  Downloading fiftyone_brain-0.8.2-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from fiftyone) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fiftyone) (0.8.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiftyone) (57.4.0)\n",
            "Collecting hypercorn>=0.13.2\n",
            "  Downloading Hypercorn-0.13.2-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.7/dist-packages (from fiftyone) (7.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fiftyone) (1.3.5)\n",
            "Collecting Deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting motor<3,>=2.3\n",
            "  Downloading motor-2.5.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting universal-analytics-python3<2,>=1.0.1\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Collecting starlette==0.16.0\n",
            "  Downloading starlette-0.16.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 322 kB/s \n",
            "\u001b[?25hCollecting fiftyone-db<0.4,>=0.3\n",
            "  Downloading fiftyone_db-0.3.0-py3-none-manylinux1_x86_64.whl (29.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.2 MB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from fiftyone) (2022.1)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
            "Collecting sseclient-py<2,>=1.7.2\n",
            "  Downloading sseclient_py-1.7.2-py2.py3-none-any.whl (8.4 kB)\n",
            "Collecting Jinja2>=3\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 59.5 MB/s \n",
            "\u001b[?25hCollecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from fiftyone) (4.5.4.60)\n",
            "Collecting pprintpp\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting argcomplete\n",
            "  Downloading argcomplete-2.0.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting sse-starlette<1,>=0.10.3\n",
            "  Downloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
            "Collecting voxel51-eta<0.8,>=0.7.0\n",
            "  Downloading voxel51_eta-0.7.0-py2.py3-none-any.whl (563 kB)\n",
            "\u001b[K     |████████████████████████████████| 563 kB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from fiftyone) (3.13)\n",
            "Collecting pymongo<4,>=3.11\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[K     |████████████████████████████████| 508 kB 10.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fiftyone) (1.0.2)\n",
            "Collecting strawberry-graphql==0.96.0\n",
            "  Downloading strawberry_graphql-0.96.0-py3-none-any.whl (135 kB)\n",
            "\u001b[K     |████████████████████████████████| 135 kB 46.1 MB/s \n",
            "\u001b[?25hCollecting plotly<5,>=4.14\n",
            "  Downloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.2 MB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from fiftyone) (5.4.8)\n",
            "Collecting retrying\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "Collecting dacite>=1.6.0\n",
            "  Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting mongoengine==0.20.0\n",
            "  Downloading mongoengine-0.20.0-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 70.2 MB/s \n",
            "\u001b[?25hCollecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9 MB 123 kB/s \n",
            "\u001b[?25hCollecting ndjson\n",
            "  Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fiftyone) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fiftyone) (21.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from fiftyone) (0.18.3)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.24.9-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fiftyone) (3.2.2)\n",
            "Collecting anyio<4,>=3.0.0\n",
            "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from starlette==0.16.0->fiftyone) (4.2.0)\n",
            "Collecting backports.cached-property<2.0.0,>=1.0.1\n",
            "  Downloading backports.cached_property-1.0.2-py3-none-any.whl (6.1 kB)\n",
            "Collecting graphql-core<3.2.0,>=3.1.0\n",
            "  Downloading graphql_core-3.1.7-py3-none-any.whl (189 kB)\n",
            "\u001b[K     |████████████████████████████████| 189 kB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<9.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from strawberry-graphql==0.96.0->fiftyone) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from strawberry-graphql==0.96.0->fiftyone) (2.8.2)\n",
            "Collecting python-multipart<0.0.6,>=0.0.5\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Requirement already satisfied: pygments<3.0,>=2.3 in /usr/local/lib/python3.7/dist-packages (from strawberry-graphql==0.96.0->fiftyone) (2.6.1)\n",
            "Collecting sentinel<0.4.0,>=0.3.0\n",
            "  Downloading sentinel-0.3.0-py3-none-any.whl (6.0 kB)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.0.0->starlette==0.16.0->fiftyone) (2.10)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from fiftyone-brain<0.9,>=0.8->fiftyone) (1.4.1)\n",
            "Collecting h2>=3.1.0\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting wsproto>=0.14.0\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting priority\n",
            "  Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
            "Collecting h11\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting hpack<5,>=4.0\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Collecting hyperframe<7,>=6.0\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3->fiftyone) (2.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly<5,>=4.14->fiftyone) (1.15.0)\n",
            "Collecting httpx>=0.10.0\n",
            "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2022.5.18.1)\n",
            "Collecting httpcore<0.16.0,>=0.15.0\n",
            "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting h11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (4.11.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (1.24.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (0.3.5.1)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (2.23.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from voxel51-eta<0.8,>=0.7.0->fiftyone) (1.5.1)\n",
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->voxel51-eta<0.8,>=0.7.0->fiftyone) (3.8.0)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.28.0,>=1.27.9\n",
            "  Downloading botocore-1.27.9-py3-none-any.whl (8.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9 MB 48.5 MB/s \n",
            "\u001b[?25hCollecting urllib3\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 78.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from Deprecated->fiftyone) (1.14.1)\n",
            "Collecting dnspython>=1.15.0\n",
            "  Downloading dnspython-2.2.1-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet>=0.3 in /usr/local/lib/python3.7/dist-packages (from eventlet->fiftyone) (1.1.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fiftyone) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fiftyone) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fiftyone) (1.4.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->voxel51-eta<0.8,>=0.7.0->fiftyone) (3.0.4)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 61.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->fiftyone) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->fiftyone) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->fiftyone) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->fiftyone) (2021.11.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fiftyone) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fiftyone) (3.1.0)\n",
            "Building wheels for collected packages: python-multipart, retrying\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=fe79862fcd763be4d48f1c23d03d0d041987d72bae15c1fecca935d762366837\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11447 sha256=9354e9bd159664e2eb587287a5e873eabc64e076e2a3d7013b2e41dae13a2f38\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
            "Successfully built python-multipart retrying\n",
            "Installing collected packages: sniffio, urllib3, rfc3986, jmespath, h11, anyio, hyperframe, httpcore, hpack, botocore, wsproto, toml, starlette, sentinel, s3transfer, retrying, python-multipart, pymongo, priority, patool, ndjson, httpx, h2, graphql-core, dnspython, backports.cached-property, argcomplete, xmltodict, voxel51-eta, universal-analytics-python3, strawberry-graphql, sseclient-py, sse-starlette, pprintpp, plotly, motor, mongoengine, kaleido, Jinja2, hypercorn, fiftyone-db, fiftyone-brain, eventlet, Deprecated, dacite, boto3, aiofiles, fiftyone\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.1.1\n",
            "    Uninstalling pymongo-4.1.1:\n",
            "      Successfully uninstalled pymongo-4.1.1\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.5.0\n",
            "    Uninstalling plotly-5.5.0:\n",
            "      Successfully uninstalled plotly-5.5.0\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Deprecated-1.2.13 Jinja2-3.1.2 aiofiles-0.8.0 anyio-3.6.1 argcomplete-2.0.0 backports.cached-property-1.0.2 boto3-1.24.9 botocore-1.27.9 dacite-1.6.0 dnspython-2.2.1 eventlet-0.33.1 fiftyone-0.16.2 fiftyone-brain-0.8.2 fiftyone-db-0.3.0 graphql-core-3.1.7 h11-0.12.0 h2-4.1.0 hpack-4.0.0 httpcore-0.15.0 httpx-0.23.0 hypercorn-0.13.2 hyperframe-6.0.1 jmespath-1.0.0 kaleido-0.2.1 mongoengine-0.20.0 motor-2.5.1 ndjson-0.3.1 patool-1.12 plotly-4.14.3 pprintpp-0.4.0 priority-2.0.0 pymongo-3.12.3 python-multipart-0.0.5 retrying-1.3.3 rfc3986-1.5.0 s3transfer-0.6.0 sentinel-0.3.0 sniffio-1.2.0 sse-starlette-0.10.3 sseclient-py-1.7.2 starlette-0.16.0 strawberry-graphql-0.96.0 toml-0.10.2 universal-analytics-python3-1.1.1 urllib3-1.25.11 voxel51-eta-0.7.0 wsproto-1.1.0 xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "uRduzwlFE7l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "dataset_train = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"train\",\n",
        "    label_types=[\"detections\"],\n",
        "    classes=['person',\n",
        " 'bicycle',\n",
        " 'car',\n",
        " 'motorcycle',\n",
        " 'airplane',\n",
        " 'bus',\n",
        " 'train',\n",
        " 'boat',\n",
        " 'bird',\n",
        " 'cat',\n",
        " 'dog',\n",
        " 'horse',\n",
        " 'sheep',\n",
        " 'cow',\n",
        " 'bottle',\n",
        " 'chair',\n",
        " 'couch',\n",
        " 'potted plant',\n",
        " 'dining table',\n",
        " 'tv'],\n",
        "     max_samples=20000,\n",
        "\n",
        ")\n",
        "# session = fo.launch_app(dataset_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EDhBx6aE--v",
        "outputId": "1d20ea93-7790-4d39-c593-cac4a5558986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n",
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n",
            "15194 images found; downloading the remaining 4806\n",
            " 100% |████████████████| 4806/4806 [3.3m elapsed, 0s remaining, 23.2 images/s]      \n",
            "Writing annotations for 20000 downloaded samples to '/root/fiftyone/coco-2017/train/labels.json'\n",
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n",
            "Loading 'coco-2017' split 'train'\n",
            " 100% |█████████████| 20000/20000 [2.7m elapsed, 0s remaining, 142.2 samples/s]      \n",
            "Dataset 'coco-2017-train-20000' created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdphDfyJgAD6",
        "outputId": "9d0f6b56-d88e-4e7f-99db-f0c119d54625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Name:        coco-2017-train-20000\n",
              "Media type:  image\n",
              "Num samples: 20000\n",
              "Persistent:  False\n",
              "Tags:        ['train']\n",
              "Sample fields:\n",
              "    id:           fiftyone.core.fields.ObjectIdField\n",
              "    filepath:     fiftyone.core.fields.StringField\n",
              "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
              "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone.utils.coco as fouc"
      ],
      "metadata": {
        "id": "gJwziTFtW9k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "OL0mS95M8F46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FiftyOneTorchDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\n",
        "    \n",
        "    Args:\n",
        "        fiftyone_dataset: a FiftyOne dataset or view that will be used for training or testing\n",
        "        transforms (None): a list of PyTorch transforms to apply to images and targets when loading\n",
        "        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset that contains the \n",
        "            desired labels to load\n",
        "        classes (None): a list of class strings that are used to define the mapping between\n",
        "            class names and indices. If None, it will use all classes present in the given fiftyone_dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        fiftyone_dataset,\n",
        "        transforms=None,\n",
        "        gt_field=\"ground_truth\",\n",
        "        classes=None,\n",
        "    ):\n",
        "        self.samples = fiftyone_dataset\n",
        "        self.transforms = transforms\n",
        "        self.gt_field = gt_field\n",
        "\n",
        "        self.img_paths = self.samples.values(\"filepath\")\n",
        "\n",
        "        self.classes = classes\n",
        "        if not self.classes:\n",
        "            # Get list of distinct labels that exist in the view\n",
        "            self.classes = self.samples.distinct(\n",
        "                \"%s.detections.label\" % gt_field\n",
        "            )\n",
        "\n",
        "        if self.classes[0] != \"background\":\n",
        "            self.classes = [\"background\"] + self.classes\n",
        "\n",
        "        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        sample = self.samples[img_path]\n",
        "        metadata = sample.metadata\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        area = []\n",
        "        iscrowd = []\n",
        "        detections = sample[self.gt_field].detections\n",
        "        for det in detections:\n",
        "            category_id = self.labels_map_rev[det.label]\n",
        "            coco_obj = fouc.COCOObject.from_label(\n",
        "                det, metadata, category_id=category_id,\n",
        "            )\n",
        "            x, y, w, h = coco_obj.bbox\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(coco_obj.category_id)\n",
        "            area.append(coco_obj.area)\n",
        "            iscrowd.append(coco_obj.iscrowd)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target[\"image_id\"] = torch.as_tensor([idx])\n",
        "        target[\"area\"] = torch.as_tensor(area, dtype=torch.float32)\n",
        "        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def get_classes(self):\n",
        "        return self.classes"
      ],
      "metadata": {
        "id": "kuYXOaBNooOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kwz5oswvXcmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset_train.first()\n",
        "print(sample.ground_truth.detections[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0u9jrN2grLC",
        "outputId": "47fed14c-5415-4c5d-a9fd-98db39e935a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Detection: {\n",
            "    'id': '62a8f0b8b473aace1e335a6b',\n",
            "    'attributes': BaseDict({}),\n",
            "    'tags': BaseList([]),\n",
            "    'label': 'potted plant',\n",
            "    'bounding_box': BaseList([\n",
            "        0.32009375,\n",
            "        0.07247663551401869,\n",
            "        0.39825,\n",
            "        0.7572897196261682,\n",
            "    ]),\n",
            "    'mask': None,\n",
            "    'confidence': None,\n",
            "    'index': None,\n",
            "    'supercategory': 'furniture',\n",
            "    'iscrowd': 0,\n",
            "}>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SequentialSampler"
      ],
      "metadata": {
        "id": "rtG5vkfrLRTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "oQO4CftkLX71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=10,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "oZSyC31QLfot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TLXRcXAyTkJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "YV3c7S6RLvmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ],
      "metadata": {
        "id": "s3uQF-QYLqFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "metadata": {
        "id": "6iNnOUNTPeKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train"
      ],
      "metadata": {
        "id": "gyyOSqGZPIZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator"
      ],
      "metadata": {
        "id": "wKaGySAHMe_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision"
      ],
      "metadata": {
        "id": "7IxPKi3JMqa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)"
      ],
      "metadata": {
        "id": "Tx3hj10lMkZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_classes = 21\n",
        "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "metadata": {
        "id": "h__ZxdE7MwbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.to(device)\n",
        "# params = [p for p in model.parameters() if p.requires_grad]\n",
        "# optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "# # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "# lr_scheduler = None\n",
        "\n",
        "# num_epochs = 2"
      ],
      "metadata": {
        "id": "TGfzkDIEL2MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(num_classes):\n",
        "    # load a model pre-trained pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "    \n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "7E4Ly5QbYA9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_2PEy-0YLrN",
        "outputId": "30f744ea-392a-4a7f-b5b9-9295b260daf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te_tXoTlaQSx",
        "outputId": "cfc5994f-cf64-4af1-cfdf-4a9f71d84bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 156554, done.\u001b[K\n",
            "remote: Counting objects: 100% (1118/1118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 156554 (delta 986), reused 1092 (delta 979), pack-reused 155436\u001b[K\n",
            "Receiving objects: 100% (156554/156554), 311.35 MiB | 25.66 MiB/s, done.\n",
            "Resolving deltas: 100% (138994/138994), done.\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be376084d version check against PyTorch's CUDA version\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import functions from the torchvision references we cloned\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "\n",
        "def do_training(model, torch_dataset, torch_dataset_test, num_epochs=4):\n",
        "    # define training and validation data loaders\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        torch_dataset, batch_size=2, shuffle=True, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "    \n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        torch_dataset_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    # train on the GPU or on the CPU, if a GPU is not available\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(\"Using device %s\" % device)\n",
        "\n",
        "    # move model to the right device\n",
        "    model.to(device)\n",
        "\n",
        "    # construct an optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                    step_size=3,\n",
        "                                                    gamma=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train for one epoch, printing every 10 iterations\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "\n",
        "        # update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        # evaluate on the test dataset\n",
        "        evaluate(model, data_loader_test, device=device)"
      ],
      "metadata": {
        "id": "lr_sXeokYDDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "busy_view = dataset_train.match(F(\"ground_truth.detections\").length() > 10)\n",
        "\n",
        "busy_torch_dataset = FiftyOneTorchDataset(busy_view)"
      ],
      "metadata": {
        "id": "itKvnlmXhv8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "My_Classes = ['person',\n",
        " 'bicycle',\n",
        " 'car',\n",
        " 'motorcycle',\n",
        " 'airplane',\n",
        " 'bus',\n",
        " 'train',\n",
        " 'boat',\n",
        " 'bird',\n",
        " 'cat',\n",
        " 'dog',\n",
        " 'horse',\n",
        " 'sheep',\n",
        " 'cow',\n",
        " 'bottle',\n",
        " 'chair',\n",
        " 'couch',\n",
        " 'potted plant',\n",
        " 'dining table',\n",
        " 'tv']\n",
        "class_view = dataset_train.filter_labels(\"ground_truth\",\n",
        "        F(\"label\").is_in(My_Classes))\n",
        "\n",
        "print(len(class_view))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY0VmIkViFpQ",
        "outputId": "41a2a788-7642-437a-d386-5eee3082005e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transforms as T\n",
        "\n",
        "train_transforms = T.Compose([T.ToTensor(), T.RandomHorizontalFlip(0.5)])\n",
        "test_transforms = T.Compose([T.ToTensor()])"
      ],
      "metadata": {
        "id": "mUznQ9bLiyGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_view = class_view.take(16000, seed=51)\n",
        "test_view = class_view.exclude([s.id for s in train_view])\n"
      ],
      "metadata": {
        "id": "GE-KQDR0i-fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_dataset = FiftyOneTorchDataset(train_view, train_transforms,\n",
        "        classes=My_Classes)\n",
        "torch_dataset_test = FiftyOneTorchDataset(test_view, test_transforms, \n",
        "        classes=My_Classes)"
      ],
      "metadata": {
        "id": "KZLr59afkBAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(len(My_Classes)+1)"
      ],
      "metadata": {
        "id": "C8OKOR-QksAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "do_training(model, torch_dataset, torch_dataset_test, num_epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "u9ccmb03kyb_",
        "outputId": "aa2ded8b-3b66-4764-e7bf-594fb41a08cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pymongo/topology.py:165: UserWarning: MongoClient opened before fork. Create MongoClient only after forking. See PyMongo's documentation for details: https://pymongo.readthedocs.io/en/stable/faq.html#is-pymongo-fork-safe\n",
            "  \"MongoClient opened before fork. Create MongoClient only \"\n",
            "/usr/local/lib/python3.7/dist-packages/pymongo/topology.py:165: UserWarning: MongoClient opened before fork. Create MongoClient only after forking. See PyMongo's documentation for details: https://pymongo.readthedocs.io/en/stable/faq.html#is-pymongo-fork-safe\n",
            "  \"MongoClient opened before fork. Create MongoClient only \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [   0/8000]  eta: 4 days, 17:33:36  lr: 0.000010  loss: 3.8570 (3.8570)  loss_classifier: 3.0781 (3.0781)  loss_box_reg: 0.0592 (0.0592)  loss_objectness: 0.6891 (0.6891)  loss_rpn_box_reg: 0.0305 (0.0305)  time: 51.1021  data: 1.8485  max mem: 0\n",
            "Epoch: [0]  [  10/8000]  eta: 4 days, 11:28:01  lr: 0.000060  loss: 3.8294 (3.8663)  loss_classifier: 3.0694 (3.0656)  loss_box_reg: 0.0226 (0.0422)  loss_objectness: 0.6907 (0.6913)  loss_rpn_box_reg: 0.0328 (0.0673)  time: 48.4207  data: 0.1846  max mem: 0\n",
            "Epoch: [0]  [  20/8000]  eta: 4 days, 7:18:33  lr: 0.000110  loss: 3.8372 (3.8880)  loss_classifier: 3.0227 (3.0043)  loss_box_reg: 0.0337 (0.0471)  loss_objectness: 0.6899 (0.6901)  loss_rpn_box_reg: 0.0678 (0.1464)  time: 46.3809  data: 0.0136  max mem: 0\n",
            "Epoch: [0]  [  30/8000]  eta: 4 days, 3:45:25  lr: 0.000160  loss: 3.5702 (3.7310)  loss_classifier: 2.7962 (2.8743)  loss_box_reg: 0.0337 (0.0413)  loss_objectness: 0.6886 (0.6893)  loss_rpn_box_reg: 0.0866 (0.1261)  time: 43.2111  data: 0.0094  max mem: 0\n",
            "Epoch: [0]  [  40/8000]  eta: 4 days, 4:28:08  lr: 0.000210  loss: 3.0716 (3.4553)  loss_classifier: 2.2866 (2.6177)  loss_box_reg: 0.0267 (0.0410)  loss_objectness: 0.6834 (0.6865)  loss_rpn_box_reg: 0.0546 (0.1100)  time: 44.2124  data: 0.0091  max mem: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0b69866f3721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-707695a30f61>\u001b[0m in \u001b[0;36mdo_training\u001b[0;34m(model, torch_dataset, torch_dataset_test, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class Averager:\n",
        "#     def __init__(self):\n",
        "#         self.current_total = 0.0\n",
        "#         self.iterations = 0.0\n",
        "\n",
        "#     def send(self, value):\n",
        "#         self.current_total += value\n",
        "#         self.iterations += 1\n",
        "\n",
        "@property\n",
        "#     def value(self):\n",
        "#         if self.iterations == 0:\n",
        "#             return 0\n",
        "#         else:\n",
        "#             return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "#     def reset(self):\n",
        "#         self.current_total = 0.0\n",
        "#         self.iterations = 0.0"
      ],
      "metadata": {
        "id": "SOzdRMzGNWol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_hist = Averager()\n",
        "# itr = 1\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     loss_hist.reset()\n",
        "    \n",
        "#     for images, targets, image_ids in train_data_loader:\n",
        "        \n",
        "#         images = list(image.to(device) for image in images)\n",
        "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "#         loss_dict = model(images, targets)\n",
        "\n",
        "#         losses = sum(loss for loss in loss_dict.values())\n",
        "#         loss_value = losses.item()\n",
        "\n",
        "#         loss_hist.send(loss_value)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         losses.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         if itr % 50 == 0:\n",
        "#             print(f\"Iteration #{itr} loss: {loss_value}\")\n",
        "\n",
        "#         itr += 1\n",
        "    \n",
        "#     # update the learning rate\n",
        "#     if lr_scheduler is not None:\n",
        "#         lr_scheduler.step()\n",
        "\n",
        "#     print(f\"Epoch #{epoch} loss: {loss_hist.value}\")  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "dnQlvxSwNQ41",
        "outputId": "9bcc0ad2-3f5f-49db-c644-339889050523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-fa1c3f5f7f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/fiftyone/core/dataset.py\", line 263, in __getitem__\n    \"Accessing dataset samples by numeric index is not supported. \"\nValueError: Accessing dataset samples by numeric index is not supported. Use sample IDs, filepaths, slices, boolean arrays, or a boolean ViewExpression instead\n"
          ]
        }
      ]
    }
  ]
}